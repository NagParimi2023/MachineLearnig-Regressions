{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X8c1ASQ1Xpr"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load the dataset (assuming it's stored in a CSV file)\n",
        "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "housing_data = pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
        "\n",
        "# Split data into features (X) and target variable (y)\n",
        "X = housing_data.drop('MEDV', axis=1)\n",
        "y = housing_data['MEDV']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('initial data :', housing_data.head())"
      ],
      "metadata": {
        "id": "z_C36r4EZYdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nAfter Scaling:' , X_train_scaled)"
      ],
      "metadata": {
        "id": "NEE2VGKtZg1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Lasso regression\n",
        "alpha = 0.1  # Regularization parameter\n",
        "lasso = Lasso(alpha=alpha)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = lasso.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Visualize feature selection\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(lasso.coef_)), lasso.coef_, marker='o', linestyle='--', color='b', label='Lasso coefficients')\n",
        "plt.axhline(y=0, color='r', linestyle='-')\n",
        "plt.title('Feature Selection with Lasso')\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Coefficient')\n",
        "plt.xticks(range(len(X.columns)), X.columns, rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Count the number of selected features\n",
        "num_selected_features = np.sum(lasso.coef_ != 0)\n",
        "print(\"Number of columns selected:\", num_selected_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "1UmtQDO-ZUGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify rejected columns\n",
        "rejected_columns = X.columns[lasso.coef_ == 0]\n",
        "\n",
        "# Explanation\n",
        "print(\"Rejected columns:\")\n",
        "for col in rejected_columns:\n",
        "    print(col, \"- This column was rejected by Lasso regression because its coefficient became zero.\")"
      ],
      "metadata": {
        "id": "NIeWkoZe17mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a range of lambda values (alpha values)\n",
        "alpha_values = [0.001, 0.01, 0.1, 0.5, 1.0, 5.0]\n",
        "\n",
        "# Lists to store results\n",
        "rmse_values = []\n",
        "selected_features = []\n",
        "\n",
        "# Loop through different lambda values\n",
        "for alpha in alpha_values:\n",
        "    # Perform Lasso regression\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict on the testing set\n",
        "    y_pred = lasso.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    rmse_values.append(rmse)\n",
        "\n",
        "    # Count selected features\n",
        "    num_selected_features = np.sum(lasso.coef_ != 0)\n",
        "    selected_features.append(num_selected_features)\n",
        "\n",
        "    print(f\"Lambda: {alpha}, RMSE: {rmse}, Number of selected features: {num_selected_features}\")\n",
        "\n",
        "# Plot RMSE vs. lambda\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(alpha_values, rmse_values, marker='o', linestyle='-', color='b')\n",
        "plt.title('RMSE vs. Lambda (Alpha)')\n",
        "plt.xlabel('Lambda (Alpha)')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xscale('log')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Number of Selected Features vs. lambda\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(alpha_values, selected_features, marker='o', linestyle='-', color='r')\n",
        "plt.title('Number of Selected Features vs. Lambda (Alpha)')\n",
        "plt.xlabel('Lambda (Alpha)')\n",
        "plt.ylabel('Number of Selected Features')\n",
        "plt.xscale('log')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4two-uWX2XsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define a range of lambda values (alpha values)\n",
        "alpha_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "\n",
        "# Lists to store results\n",
        "rmse_values = []\n",
        "selected_features = []\n",
        "\n",
        "# Loop through different lambda values\n",
        "for alpha in alpha_values:\n",
        "    # Perform Ridge regression\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict on the testing set\n",
        "    y_pred = ridge.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    rmse_values.append(rmse)\n",
        "\n",
        "    # Count selected features (Ridge regression does not perform feature selection)\n",
        "    num_selected_features = X_train.shape[1]\n",
        "    selected_features.append(num_selected_features)\n",
        "\n",
        "    print(f\"Lambda: {alpha}, RMSE: {rmse}\")\n",
        "\n",
        "# Plot RMSE vs. lambda\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(alpha_values, rmse_values, marker='o', linestyle='-', color='b')\n",
        "plt.title('RMSE vs. Lambda (Alpha) - Ridge Regression')\n",
        "plt.xlabel('Lambda (Alpha)')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xscale('log')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dfo4t3Pq3Ikb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define alpha and l1_ratio values\n",
        "alpha = 0.1\n",
        "l1_ratio = 0.5\n",
        "\n",
        "# Perform Elastic Net regression\n",
        "elastic_net = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "elastic_net.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = elastic_net.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Print the coefficients\n",
        "coefficients = pd.DataFrame({'feature': X.columns, 'coefficient': elastic_net.coef_})\n",
        "print(coefficients)"
      ],
      "metadata": {
        "id": "Ee36Bh-X4CLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract feature names and coefficients\n",
        "feature_names = X.columns\n",
        "coefficients = elastic_net.coef_\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, coefficients, color='skyblue')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Coefficients - Elastic Net')\n",
        "plt.grid(axis='x')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dnq51l5a4I_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Function to generate synthetic datasets\n",
        "def generate_data(n_samples, noise):\n",
        "    np.random.seed(0)\n",
        "    X = np.linspace(0, 1, n_samples).reshape(-1, 1)\n",
        "    y_true = 3 * X.ravel()  # True function: y = 3x\n",
        "    y_noisy = y_true + np.random.normal(scale=noise, size=X.shape[0])  # Add noise\n",
        "    return X, y_noisy\n",
        "\n",
        "# Generate synthetic datasets for each scenario\n",
        "X_high_bias_low_variance, y_high_bias_low_variance = generate_data(100, 2)\n",
        "X_low_bias_high_variance, y_low_bias_high_variance = generate_data(100, 10)\n",
        "X_perfect_fit, y_perfect_fit = generate_data(100, 2)\n",
        "\n",
        "# Introduce outliers for the low bias and high variance scenario\n",
        "X_low_bias_high_variance = np.append(X_low_bias_high_variance, [[0.1], [0.2]], axis=0)\n",
        "y_low_bias_high_variance = np.append(y_low_bias_high_variance, [40, 50])\n",
        "\n",
        "# Define a function to fit linear regression models and plot results\n",
        "def fit_linear_model(X, y_true, title):\n",
        "    plt.scatter(X, y_true, color='blue', label='True function')\n",
        "\n",
        "    # Create linear regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X, y_true)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # Plot true function and predicted line\n",
        "    plt.plot(X, y_pred, color='red', label=f'Predicted function (MSE={mse:.2f})')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.legend()\n",
        "\n",
        "# Plot all three scenarios\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "fit_linear_model(X_high_bias_low_variance, y_high_bias_low_variance, 'High Bias and Low Variance')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "fit_linear_model(X_low_bias_high_variance, y_low_bias_high_variance, 'Low Bias and High Variance')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "fit_linear_model(X_perfect_fit, y_perfect_fit, 'Perfect Fit')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9CvjfaNq5OmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Function to generate synthetic datasets\n",
        "def generate_data(n_samples, noise):\n",
        "    np.random.seed(0)\n",
        "    X = np.linspace(0, 1, n_samples).reshape(-1, 1)\n",
        "    y_true = 3 * X.ravel()  # True function: y = 3x\n",
        "    y_noisy = y_true + np.random.normal(scale=noise, size=X.shape[0])  # Add noise\n",
        "    return X, y_noisy\n",
        "\n",
        "# Generate synthetic datasets for each scenario\n",
        "X_high_bias_low_variance, y_high_bias_low_variance = generate_data(100, 2)\n",
        "X_low_bias_high_variance, y_low_bias_high_variance = generate_data(100, 10)\n",
        "X_perfect_fit, y_perfect_fit = generate_data(100, 2)\n",
        "\n",
        "# Introduce outliers for the low bias and high variance scenario\n",
        "X_low_bias_high_variance = np.append(X_low_bias_high_variance, [[0.1], [0.2]], axis=0)\n",
        "y_low_bias_high_variance = np.append(y_low_bias_high_variance, [40, 50])\n",
        "\n",
        "# Define a function to fit linear regression models and plot results\n",
        "def fit_linear_model(X, y_true, title):\n",
        "    plt.scatter(X, y_true, color='blue', label='True function')\n",
        "\n",
        "    # Create linear regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X, y_true)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # Plot true function and predicted line\n",
        "    plt.plot(X, y_pred, color='red', label=f'Predicted function (MSE={mse:.2f})')\n",
        "\n",
        "    plt.title(title + f'\\nTrain MSE: {mse:.2f}')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.legend()\n",
        "\n",
        "# Plot all three scenarios\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "fit_linear_model(X_high_bias_low_variance, y_high_bias_low_variance, 'High Bias and Low Variance')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "fit_linear_model(X_low_bias_high_variance, y_low_bias_high_variance, 'Low Bias and High Variance')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "fit_linear_model(X_perfect_fit, y_perfect_fit, 'Perfect Fit')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gVflhpzNdDZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to generate synthetic datasets\n",
        "def generate_data(n_samples, noise):\n",
        "    np.random.seed(0)\n",
        "    X = np.linspace(0, 1, n_samples).reshape(-1, 1)\n",
        "    y_true = 3 * X.ravel()  # True function: y = 3x\n",
        "    y_noisy = y_true + np.random.normal(scale=noise, size=X.shape[0])  # Add noise\n",
        "    return X, y_noisy\n",
        "\n",
        "# Generate synthetic datasets for each scenario\n",
        "X_high_bias_low_variance, y_high_bias_low_variance = generate_data(100, 2)\n",
        "X_low_bias_high_variance, y_low_bias_high_variance = generate_data(100, 10)\n",
        "X_perfect_fit, y_perfect_fit = generate_data(100, 2)\n",
        "\n",
        "# Introduce outliers for the low bias and high variance scenario\n",
        "X_low_bias_high_variance = np.append(X_low_bias_high_variance, [[0.1], [0.2]], axis=0)\n",
        "y_low_bias_high_variance = np.append(y_low_bias_high_variance, [40, 50])\n",
        "\n",
        "# Define a function to fit linear regression models, calculate MSE, and plot results\n",
        "def fit_linear_model(X, y_true, title):\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Create linear regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    # Calculate MSE for training and testing\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "    # Plot true function and predicted line\n",
        "    plt.scatter(X, y_true, color='blue', label='True function')\n",
        "    plt.plot(X, model.predict(X), color='red', label=f'Predicted function (Train MSE={mse_train:.2f}, Test MSE={mse_test:.2f})')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.legend()\n",
        "\n",
        "# Plot all three scenarios\n",
        "plt.figure(figsize=(25, 10))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "fit_linear_model(X_high_bias_low_variance, y_high_bias_low_variance, 'High Bias and Low Variance')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "fit_linear_model(X_low_bias_high_variance, y_low_bias_high_variance, 'Low Bias and High Variance')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5-pNX5QEdbWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile linear.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self):\n",
        "        self.coefficients = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        # Add bias term\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "        # Compute coefficients using normal equation\n",
        "        self.coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Add bias term\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "        return X @ self.coefficients\n",
        "\n",
        "    def mean_squared_error(self, y_true, y_pred):\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "\n",
        "# Now let's write tests using pytest\n",
        "\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def linear_regression():\n",
        "    return LinearRegression()\n",
        "\n",
        "def test_linear_regression_train_predict(linear_regression):\n",
        "    X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "    y_train = np.array([2, 4, 5, 4, 5])\n",
        "    linear_regression.train(X_train, y_train)\n",
        "\n",
        "    X_test = np.array([[6], [7], [8]])\n",
        "    y_pred = linear_regression.predict(X_test)\n",
        "\n",
        "    assert len(linear_regression.coefficients) == 2  # Check coefficients shape\n",
        "    assert y_pred.shape == (3,)  # Check predictions shape\n",
        "\n",
        "def test_linear_regression_mean_squared_error(linear_regression):\n",
        "    y_true = np.array([2, 4, 5, 4, 5])\n",
        "    y_pred = np.array([2.2, 3.8, 5.1, 4.3, 4.9])\n",
        "\n",
        "    mse = linear_regression.mean_squared_error(y_true, y_pred)\n",
        "    assert np.isclose(mse, 0.037, atol=1e-3)  # Check mean squared error\n"
      ],
      "metadata": {
        "id": "57BATrY681II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytest pytest-sugar"
      ],
      "metadata": {
        "id": "ZKDTZ2Nwfe2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest linear.py -v"
      ],
      "metadata": {
        "id": "61KMkMqMmTyE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}